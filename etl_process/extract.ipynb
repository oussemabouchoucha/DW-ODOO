{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e54392",
   "metadata": {},
   "source": [
    "# Data Extraction Process\n",
    "\n",
    "This notebook extracts data from source systems and loads it into the Bronze layer. It represents the first step in our ETL pipeline.\n",
    "\n",
    "## Process Overview\n",
    "1. Set up environment and initialize Spark session\n",
    "2. Define paths and configure logging\n",
    "3. Extract data from source files\n",
    "4. Load data into Bronze layer\n",
    "5. Log results and clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cfd0b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Initialize Spark session and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME environment variable\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jre1.8.0_451'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Ingestion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "path-config-header",
   "metadata": {},
   "source": [
    "## 2. Path Configuration\n",
    "\n",
    "Define paths for source data, bronze layer, and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521aa602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"../data/\"\n",
    "bronze_base_path = \"../output/bronzeLayer\"\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Log path organized by date\n",
    "log_dir = os.path.join(\"../logs\", \"data_ingestion\", date_str)\n",
    "log_path = os.path.join(log_dir, \"data_ingestion.log\")\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logging-header",
   "metadata": {},
   "source": [
    "## 3. Logging Configuration\n",
    "\n",
    "Set up logging to track the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3bd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to write to the log file\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"Logs messages with the specified level.\"\"\"\n",
    "    if level == \"info\":\n",
    "        logging.info(message)\n",
    "    elif level == \"error\":\n",
    "        logging.error(message)\n",
    "    print(message)  \n",
    "\n",
    "# Define source\n",
    "source = \"mrp_production\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraction-header",
   "metadata": {},
   "source": [
    "## 4. Data Extraction and Loading\n",
    "\n",
    "Extract data from source files and load it into the Bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "main-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ingestion for mrp_production from ../data/mrp_production.csv to ../output/bronzeLayer\\2025-05-08\n",
      "Successfully ingested mrp_production data to ../output/bronzeLayer\\2025-05-08\n",
      "Data ingestion process completed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raw_path = os.path.join(base_path, f\"{source}.csv\")\n",
    "    bronze_path = os.path.join(bronze_base_path, date_str)\n",
    "\n",
    "    # Log start of the ingestion process\n",
    "    log_message(f\"Starting ingestion for {source} from {raw_path} to {bronze_path}\")\n",
    "\n",
    "    # Read raw data\n",
    "    df = spark.read.csv(raw_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Ensure the bronze path exists\n",
    "    os.makedirs(bronze_path, exist_ok=True)\n",
    "    \n",
    "    # Write to Bronze layer in Parquet format, organized by date\n",
    "    df.write.mode(\"overwrite\").parquet(bronze_path)\n",
    "    \n",
    "    # Log successful ingestion\n",
    "    log_message(f\"Successfully ingested {source} data to {bronze_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log any errors encountered during ingestion\n",
    "    log_message(f\"Error ingesting {source} data: {e}\", level=\"error\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "log_message(\"Data ingestion process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "The extraction process has successfully:\n",
    "- Read data from the source CSV file\n",
    "- Loaded it into the Bronze layer in Parquet format\n",
    "- Organized the data by date\n",
    "- Logged all operations for traceability\n",
    "\n",
    "The data is now ready for the next step in the ETL pipeline: data auditing and transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
