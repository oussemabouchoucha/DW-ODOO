{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Audit Report\n",
    "\n",
    "This notebook generates an audit report for data in the Bronze layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and set environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME environment variable\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jre1.8.0_451'\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pyspark.sql.functions as F\n",
    "import re\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session\n",
    "\n",
    "Create or reuse an existing Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing Spark session\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "try:\n",
    "    # Try to get an existing Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Data Audit\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Test if the session is active\n",
    "    spark.sparkContext.getConf().getAll()\n",
    "    print(\"Using existing Spark session\")\n",
    "except:\n",
    "    # If there's an error, create a new session\n",
    "    print(\"Creating new Spark session\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Data Audit\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Paths and Logging\n",
    "\n",
    "Set up paths for data, logs, and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "base_dir = os.path.dirname(current_dir)  # Go up one level to reach the root directory\n",
    "\n",
    "# Get the bronze layer path\n",
    "bronze_base_path = os.path.join(base_dir, \"output\", \"bronzeLayer\")\n",
    "\n",
    "# Use current date for logs\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Log path organized by date\n",
    "log_dir = os.path.join(base_dir, \"logs\", \"data_audit\", date_str)\n",
    "log_path = os.path.join(log_dir, \"data_audit.log\")\n",
    "report_path = os.path.join(log_dir, \"audit_report.txt\")\n",
    "json_report_path = os.path.join(log_dir, \"audit_report.json\")\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"Logs messages with the specified level.\"\"\"\n",
    "    if level == \"info\":\n",
    "        logging.info(message)\n",
    "    elif level == \"error\":\n",
    "        logging.error(message)\n",
    "    print(message)  # Also print to console for real-time feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Locate Latest Data\n",
    "\n",
    "Find the most recent data in the Bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest folder: c:\\Users\\oussema\\OneDrive\\Bureau\\FSEGN\\DW-Odoo\\output\\bronzeLayer\\2025-05-08\n"
     ]
    }
   ],
   "source": [
    "# Find the latest date folder in bronze layer\n",
    "date_folders = glob.glob(os.path.join(bronze_base_path, \"*\"))\n",
    "if not date_folders:\n",
    "    log_message(f\"No date folders found in {bronze_base_path}\", level=\"error\")\n",
    "else:\n",
    "    latest_folder = max(date_folders)\n",
    "    print(f\"Latest folder: {latest_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Examine Data\n",
    "\n",
    "Read the data from the Bronze layer and display basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: mrp_production\n",
      "Path: c:\\Users\\oussema\\OneDrive\\Bureau\\FSEGN\\DW-Odoo\\output\\bronzeLayer\\2025-05-08\n",
      "Row count: 1734\n",
      "Column count: 14\n",
      "Columns: Component Status, Product, Reference, Source, Responsible, Start, End, Deadline, State, Quantity Producing, Quantity To Produce, Total Quantity, Product/Cost, Product/Sales Price\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "+----------------+----------------------+-----------+-------+---------------------+-------------------+-------------------+-------------------+---------+------------------+-------------------+--------------+------------+-------------------+\n",
      "|Component Status|Product               |Reference  |Source |Responsible          |Start              |End                |Deadline           |State    |Quantity Producing|Quantity To Produce|Total Quantity|Product/Cost|Product/Sales Price|\n",
      "+----------------+----------------------+-----------+-------+---------------------+-------------------+-------------------+-------------------+---------+------------------+-------------------+--------------+------------+-------------------+\n",
      "|NULL            |[FURN_8855] Drawer    |WH/MO/01679|NULL   |OdooBot              |2024-01-03 01:00:00|2024-02-19 01:00:00|2024-01-25 01:00:00|Cancelled|12.0              |17.0               |17.0          |100.0       |110.5              |\n",
      "|NULL            |[FURN_8522] Table Top |WH/MO/07299|SO-8299|Default User Template|2024-01-03 01:00:00|2024-02-27 01:00:00|2024-01-22 01:00:00|Cancelled|4.0               |13.0               |13.0          |240.0       |380.0              |\n",
      "|Available       |[FURN_9666] Table     |WH/MO/02264|NULL   |Public user          |2024-01-06 01:00:00|2024-02-06 01:00:00|2024-03-29 01:00:00|NULL     |19.0              |19.0               |19.0          |290.0       |520.0              |\n",
      "|Available       |[FURN_7023] Wood Panel|WH/MO/03734|NULL   |Mitchell Admin       |2024-01-06 01:00:00|2024-02-17 01:00:00|2024-03-20 01:00:00|NULL     |10.0              |10.0               |10.0          |80.0        |100.0              |\n",
      "|Available       |[FURN_7023] Wood Panel|WH/MO/03614|SO-4614|Mitchell Admin       |2024-01-07 01:00:00|2024-02-03 01:00:00|2024-03-11 01:00:00|NULL     |15.0              |15.0               |15.0          |80.0        |100.0              |\n",
      "+----------------+----------------------+-----------+-------+---------------------+-------------------+-------------------+-------------------+---------+------------------+-------------------+--------------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "try:\n",
    "    # Read data from Bronze layer (Parquet format)\n",
    "    df = spark.read.parquet(latest_folder)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"Source: mrp_production\")\n",
    "    print(f\"Path: {latest_folder}\")\n",
    "    print(f\"Row count: {df.count()}\")\n",
    "    print(f\"Column count: {len(df.columns)}\")\n",
    "    print(f\"Columns: {', '.join(df.columns)}\\n\")\n",
    "    \n",
    "    # Show a sample of the data\n",
    "    print(\"Sample data (first 5 rows):\")\n",
    "    df.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    log_message(f\"Error reading data: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Data Types and Missing Values\n",
    "\n",
    "Analyze data types and identify missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Component Status has type string\n",
      "Column Product has type string\n",
      "Column Reference has type string\n",
      "Column Source has type string\n",
      "Column Responsible has type string\n",
      "Column Start has type timestamp\n",
      "Column End has type timestamp\n",
      "Column Deadline has type timestamp\n",
      "Column State has type string\n",
      "Column Quantity Producing has type double\n",
      "Column Quantity To Produce has type double\n",
      "Column Total Quantity has type double\n",
      "Column Product/Cost has type double\n",
      "Column Product/Sales Price has type double\n",
      "\n",
      "Missing Values:\n",
      " - Component Status: 526 missing values (30.33%)\n",
      " - Source: 1226 missing values (70.70%)\n",
      " - State: 325 missing values (18.74%)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "try:\n",
    "    # Create a list to hold the expressions for each column\n",
    "    missing_value_exprs = []\n",
    "    for c in df.columns:\n",
    "        # Get the data type of the column\n",
    "        col_type = df.schema[c].dataType.typeName()\n",
    "        print(f\"Column {c} has type {col_type}\")\n",
    "        \n",
    "        # For numeric types, check both null and NaN\n",
    "        if col_type in ['double', 'float']:\n",
    "            expr = F.count(F.when(F.col(c).isNull() | F.isnan(F.col(c)), c)).alias(c)\n",
    "        else:\n",
    "            # For non-numeric types, only check for null\n",
    "            expr = F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "        \n",
    "        missing_value_exprs.append(expr)\n",
    "    \n",
    "    missing_values = df.select(missing_value_exprs)\n",
    "    missing_values_dict = missing_values.collect()[0].asDict()\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    for column, count in missing_values_dict.items():\n",
    "        if count > 0:\n",
    "            print(f\" - {column}: {count} missing values ({(count/df.count())*100:.2f}%)\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error checking missing values: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check for Duplicate Rows\n",
    "\n",
    "Identify duplicate records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows: 0 (0.00% of total)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "try:\n",
    "    duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "    print(f\"Duplicate Rows: {duplicate_count} ({(duplicate_count/df.count())*100:.2f}% of total)\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error checking duplicates: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Audit Report\n",
    "\n",
    "Create and save a comprehensive audit report in both text and JSON formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audit report saved to: c:\\Users\\oussema\\OneDrive\\Bureau\\FSEGN\\DW-Odoo\\logs\\data_audit\\2025-05-08\\audit_report.txt\n",
      "JSON report saved to: c:\\Users\\oussema\\OneDrive\\Bureau\\FSEGN\\DW-Odoo\\logs\\data_audit\\2025-05-08\\audit_report.json\n"
     ]
    }
   ],
   "source": [
    "# Generate and save the audit report\n",
    "try:\n",
    "    # Create the report\n",
    "    report = []\n",
    "    report.append(f\"Audit Report for mrp_production\")\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Row Count: {df.count()}\")\n",
    "    report.append(f\"Column Count: {len(df.columns)}\")\n",
    "    report.append(f\"Columns: {', '.join(df.columns)}\\n\")\n",
    "    \n",
    "    # Add missing values to report\n",
    "    report.append(\"Missing Values:\")\n",
    "    for column, count in missing_values_dict.items():\n",
    "        if count > 0:\n",
    "            report.append(f\" - {column}: {count} missing values ({(count/df.count())*100:.2f}%)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Add duplicates to report\n",
    "    report.append(f\"Duplicate Rows: {duplicate_count} ({(duplicate_count/df.count())*100:.2f}% of total)\\n\")\n",
    "    \n",
    "    # Save the report to a text file\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(report))\n",
    "    \n",
    "    # Create JSON report\n",
    "    json_report = {\n",
    "        \"source\": \"mrp_production\",\n",
    "        \"row_count\": df.count(),\n",
    "        \"column_count\": len(df.columns),\n",
    "        \"missing_values\": missing_values_dict,\n",
    "        \"duplicate_rows\": duplicate_count\n",
    "    }\n",
    "    \n",
    "    # Save JSON report\n",
    "    with open(json_report_path, \"w\") as f:\n",
    "        json.dump([json_report], f, indent=4)\n",
    "    \n",
    "    print(f\"\\nAudit report saved to: {report_path}\")\n",
    "    print(f\"JSON report saved to: {json_report_path}\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error generating report: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Audit Report\n",
    "\n",
    "Show the generated audit report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit Report for mrp_production\n",
      "Generated on: 2025-05-08 22:31:52\n",
      "Row Count: 1734\n",
      "Column Count: 14\n",
      "Columns: Component Status, Product, Reference, Source, Responsible, Start, End, Deadline, State, Quantity Producing, Quantity To Produce, Total Quantity, Product/Cost, Product/Sales Price\n",
      "\n",
      "Missing Values:\n",
      " - Component Status: 526 missing values (30.33%)\n",
      " - Source: 1226 missing values (70.70%)\n",
      " - State: 325 missing values (18.74%)\n",
      "\n",
      "Duplicate Rows: 0 (0.00% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the audit report\n",
    "try:\n",
    "    with open(report_path, \"r\") as f:\n",
    "        report_content = f.read()\n",
    "    print(report_content)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading report: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "\n",
    "Clean up resources by stopping the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# Safely stop Spark session\n",
    "try:\n",
    "    # Check if spark session is active before stopping\n",
    "    spark.sparkContext.getConf().getAll()\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")\n",
    "except:\n",
    "    print(\"No active Spark session to stop.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
