{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data to SQL Server\n",
    "\n",
    "This notebook loads the transformed data from the Silver layer into SQL Server using a dimensional model with dimension and fact tables.\n",
    "\n",
    "## Process Overview\n",
    "1. Set up environment and initialize Spark session\n",
    "2. Read data from the Silver layer\n",
    "3. Create dimension tables (DimDate, DimDepartment, DimProductCategory, DimProduct)\n",
    "4. Create fact table (FactProduction)\n",
    "5. Load all tables to SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Initialize Spark session and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME environment variable\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jre1.8.0_451'\n",
    "\n",
    "# Import required libraries\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyodbc\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "# Configure logging\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "log_dir = os.path.join(\"../logs\", \"sql_load\", date_str)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_path = os.path.join(log_dir, \"sql_load.log\")\n",
    "\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log_message(message, level=\"info\"):\n",
    "    \"\"\"Logs messages with the specified level.\"\"\"\n",
    "    if level == \"info\":\n",
    "        logging.info(message)\n",
    "    elif level == \"error\":\n",
    "        logging.error(message)\n",
    "    print(message)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load to SQL Server\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:/spark-3.4.3/sqljdbc_4.2.8112.200_enu/sqljdbc_4.2/enu/jre8/sqljdbc42.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "log_message(\"Environment setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Path Configuration and Data Loading\n",
    "\n",
    "Define paths and load data from the Silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "base_dir = os.path.dirname(current_dir)  # Go up one level to reach the root directory\n",
    "silver_base_path = os.path.join(base_dir, \"output\", \"silverLayer\")\n",
    "\n",
    "# Find the latest date folder in silver layer\n",
    "silver_date_folders = glob.glob(os.path.join(silver_base_path, \"*\"))\n",
    "if not silver_date_folders:\n",
    "    raise Exception(f\"No date folders found in {silver_base_path}\")\n",
    "\n",
    "latest_silver_folder = max(silver_date_folders)\n",
    "silver_path = latest_silver_folder\n",
    "\n",
    "print(f\"Silver path: {silver_path}\")\n",
    "\n",
    "# Read the silver layer data\n",
    "try:\n",
    "    # Read data from silver layer\n",
    "    silver_df = spark.read.parquet(os.path.join(silver_path, \"mrp_production.parquet\"))\n",
    "    log_message(f\"Successfully read data from {os.path.join(silver_path, 'mrp_production.parquet')}\")\n",
    "    print(f\"Row count: {silver_df.count()}\")\n",
    "    print(f\"Column count: {len(silver_df.columns)}\")\n",
    "    \n",
    "    # Display column names\n",
    "    print(\"\\nColumn names in the dataset:\")\n",
    "    for col_name in silver_df.columns:\n",
    "        print(f\"- {col_name}\")\n",
    "    \n",
    "    # Convert to pandas for easier processing\n",
    "    df = silver_df.toPandas()\n",
    "    log_message(\"Converted to pandas DataFrame\")\n",
    "    \n",
    "    # Display sample data\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    log_message(f\"Error reading silver layer data: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Server Connection Setup\n",
    "\n",
    "Configure connection to SQL Server and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Server connection parameters\n",
    "server = 'localhost'\n",
    "database = 'DW_ODOO'\n",
    "trusted_connection = 'yes'  # Windows authentication\n",
    "\n",
    "# Create connection string\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection={trusted_connection}'\n",
    "\n",
    "# Function to execute SQL query\n",
    "def execute_sql(query, params=None):\n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        cursor = conn.cursor()\n",
    "        if params:\n",
    "            cursor.execute(query, params)\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error executing SQL: {str(e)}\", level=\"error\")\n",
    "        return False\n",
    "\n",
    "# Function to load data to SQL Server\n",
    "def load_dataframe_to_sql(df, table_name, if_exists='append'):\n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Generate insert statements\n",
    "        for index, row in df.iterrows():\n",
    "            columns = ', '.join(df.columns)\n",
    "            placeholders = ', '.join(['?' for _ in range(len(df.columns))])\n",
    "            query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            \n",
    "            # Convert any NaN values to None\n",
    "            values = [None if pd.isna(val) else val for val in row]\n",
    "            \n",
    "            cursor.execute(query, values)\n",
    "            \n",
    "            # Commit every 1000 rows to avoid memory issues\n",
    "            if index % 1000 == 0:\n",
    "                conn.commit()\n",
    "                print(f\"Committed {index} rows to {table_name}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        log_message(f\"Successfully loaded {len(df)} rows to {table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error loading data to {table_name}: {str(e)}\", level=\"error\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Successfully connected to SQL Server\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to SQL Server: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and Load Dimension Tables\n",
    "\n",
    "Process the data and create dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create DimDate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DimDate\n",
    "log_message(\"Creating DimDate table...\")\n",
    "\n",
    "# Get all dates from the dataset (Start, End, Deadline)\n",
    "all_dates = pd.concat([\n",
    "    df['Start'].dropna(),\n",
    "    df['End'].dropna(),\n",
    "    df['Deadline'].dropna()\n",
    "]).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "# Create date dimension\n",
    "dim_date = pd.DataFrame({\n",
    "    'Date': all_dates,\n",
    "    'Year': all_dates.dt.year,\n",
    "    'Quarter': all_dates.dt.quarter,\n",
    "    'Month': all_dates.dt.month,\n",
    "    'MonthName': all_dates.dt.strftime('%B'),\n",
    "    'Day': all_dates.dt.day,\n",
    "    'DayOfWeek': all_dates.dt.dayofweek + 1,  # 1-7 instead of 0-6\n",
    "    'DayOfWeekName': all_dates.dt.strftime('%A')\n",
    "})\n",
    "\n",
    "# Create DateKey (YYYYMMDD format)\n",
    "dim_date['DateKey'] = dim_date['Date'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# Reorder columns\n",
    "dim_date = dim_date[['DateKey', 'Date', 'Year', 'Quarter', 'Month', 'MonthName', 'Day', 'DayOfWeek', 'DayOfWeekName']]\n",
    "\n",
    "# Display sample data\n",
    "display(dim_date.head())\n",
    "print(f\"Total dates: {len(dim_date)}\")\n",
    "\n",
    "# Load DimDate to SQL\n",
    "# load_dataframe_to_sql(dim_date, 'dbo.DimDate', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create DimDepartment Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DimDepartment\n",
    "log_message(\"Creating DimDepartment table...\")\n",
    "\n",
    "# Get unique departments\n",
    "departments = df['Responsible'].dropna().unique()\n",
    "dim_department = pd.DataFrame({\n",
    "    'DepartmentID': [f\"DEPT_{i+1}\" for i in range(len(departments))],\n",
    "    'DepartmentName': departments\n",
    "})\n",
    "\n",
    "# Display the department dimension\n",
    "display(dim_department)\n",
    "\n",
    "# Create a mapping of department names to keys\n",
    "department_mapping = {}\n",
    "for i, dept in enumerate(departments):\n",
    "    department_mapping[dept] = i + 1  # DepartmentKey starts at 1\n",
    "\n",
    "# Load DimDepartment to SQL\n",
    "# load_dataframe_to_sql(dim_department, 'dbo.DimDepartment', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create DimProductCategory Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DimProductCategory\n",
    "log_message(\"Creating DimProductCategory table...\")\n",
    "\n",
    "# Get unique product categories\n",
    "product_categories = df['Product_Category'].dropna().unique()\n",
    "dim_product_category = pd.DataFrame({\n",
    "    'ProductCategoryName': product_categories\n",
    "})\n",
    "\n",
    "# Display the product category dimension\n",
    "display(dim_product_category)\n",
    "\n",
    "# Create a mapping of category names to keys\n",
    "category_mapping = {}\n",
    "for i, cat in enumerate(product_categories):\n",
    "    category_mapping[cat] = i + 1  # ProductCategoryKey starts at 1\n",
    "\n",
    "# Load DimProductCategory to SQL\n",
    "# load_dataframe_to_sql(dim_product_category, 'dbo.DimProductCategory', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create DimProduct Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DimProduct\n",
    "log_message(\"Creating DimProduct table...\")\n",
    "\n",
    "# Get unique products\n",
    "products = df[['Product_Code', 'Product_Name', 'Product_Category', 'Product_Cost', \n",
    "              'Product_Sales_Price', 'Profit_Margin_Percent', 'Price_Category', 'Margin_Category']].drop_duplicates()\n",
    "\n",
    "# Add ProductCategoryKey\n",
    "products['ProductCategoryKey'] = products['Product_Category'].map(category_mapping)\n",
    "\n",
    "# Rename columns to match SQL table\n",
    "dim_product = products.rename(columns={\n",
    "    'Product_Code': 'ProductCode',\n",
    "    'Product_Name': 'ProductName',\n",
    "    'Product_Cost': 'ProductCost',\n",
    "    'Product_Sales_Price': 'ProductSalesPrice',\n",
    "    'Profit_Margin_Percent': 'ProfitMarginPercent',\n",
    "    'Price_Category': 'PriceCategory',\n",
    "    'Margin_Category': 'MarginCategory'\n",
    "})\n",
    "\n",
    "# Select only the columns we need\n",
    "dim_product = dim_product[['ProductCode', 'ProductName', 'ProductCategoryKey', 'ProductCost', \n",
    "                          'ProductSalesPrice', 'ProfitMarginPercent', 'PriceCategory', 'MarginCategory']]\n",
    "\n",
    "# Display the product dimension\n",
    "display(dim_product.head())\n",
    "print(f\"Total products: {len(dim_product)}\")\n",
    "\n",
    "# Create a mapping of product codes to keys\n",
    "product_mapping = {}\n",
    "for i, code in enumerate(products['Product_Code'].unique()):\n",
    "    product_mapping[code] = i + 1  # ProductKey starts at 1\n",
    "\n",
    "# Load DimProduct to SQL\n",
    "# load_dataframe_to_sql(dim_product, 'dbo.DimProduct', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Load Fact Table\n",
    "\n",
    "Process the data and create the fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FactProduction\n",
    "log_message(\"Creating FactProduction table...\")\n",
    "\n",
    "# Create date key mapping function\n",
    "def date_to_key(date):\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    return int(date.strftime('%Y%m%d'))\n",
    "\n",
    "# Create the fact table\n",
    "fact_production = df[['Reference', 'Product_Code', 'Responsible', 'Start', 'End', 'Deadline',\n",
    "                     'State', 'Quantity_Producing', 'Quantity_To_Produce', 'Total_Quantity',\n",
    "                     'Production_Efficiency', 'Production_Duration_Days']].copy()\n",
    "\n",
    "# Map dimension keys\n",
    "fact_production['ProductKey'] = fact_production['Product_Code'].map(product_mapping)\n",
    "fact_production['DepartmentKey'] = fact_production['Responsible'].map(department_mapping)\n",
    "fact_production['StartDateKey'] = fact_production['Start'].apply(date_to_key)\n",
    "fact_production['EndDateKey'] = fact_production['End'].apply(date_to_key)\n",
    "fact_production['DeadlineDateKey'] = fact_production['Deadline'].apply(date_to_key)\n",
    "\n",
    "# Drop original columns that have been mapped\n",
    "fact_production = fact_production.drop(['Product_Code', 'Responsible', 'Start', 'End', 'Deadline'], axis=1)\n",
    "\n",
    "# Rename remaining columns\n",
    "fact_production = fact_production.rename(columns={\n",
    "    'Quantity_Producing': 'QuantityProducing',\n",
    "    'Quantity_To_Produce': 'QuantityToProduce',\n",
    "    'Total_Quantity': 'TotalQuantity',\n",
    "    'Production_Efficiency': 'ProductionEfficiency',\n",
    "    'Production_Duration_Days': 'ProductionDurationDays'\n",
    "})\n",
    "\n",
    "# Display the fact table\n",
    "display(fact_production.head())\n",
    "print(f\"Total fact records: {len(fact_production)}\")\n",
    "\n",
    "# Load FactProduction to SQL\n",
    "# load_dataframe_to_sql(fact_production, 'dbo.FactProduction', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute the Data Loading\n",
    "\n",
    "Uncomment the loading commands and execute them to load the data to SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables to SQL Server\n",
    "try:\n",
    "    # Load dimension tables\n",
    "    print(\"Loading DimDate...\")\n",
    "    load_dataframe_to_sql(dim_date, 'dbo.DimDate', if_exists='replace')\n",
    "    \n",
    "    print(\"Loading DimDepartment...\")\n",
    "    load_dataframe_to_sql(dim_department, 'dbo.DimDepartment', if_exists='replace')\n",
    "    \n",
    "    print(\"Loading DimProductCategory...\")\n",
    "    load_dataframe_to_sql(dim_product_category, 'dbo.DimProductCategory', if_exists='replace')\n",
    "    \n",
    "    print(\"Loading DimProduct...\")\n",
    "    load_dataframe_to_sql(dim_product, 'dbo.DimProduct', if_exists='replace')\n",
    "    \n",
    "    # Load fact table\n",
    "    print(\"Loading FactProduction...\")\n",
    "    load_dataframe_to_sql(fact_production, 'dbo.FactProduction', if_exists='replace')\n",
    "    \n",
    "    log_message(\"All tables loaded successfully to SQL Server!\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Error loading tables to SQL Server: {str(e)}\", level=\"error\")\n",
    "    import traceback\n",
    "    log_message(traceback.format_exc(), level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Stop the Spark session and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "log_message(\"Spark session stopped.\")\n",
    "log_message(\"Data loading process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
